# Google---American-Sign-Language-Fingerspelling-Recognition
Train fast and accurate American Sign Language fingerspelling recognition models




Description
Goal of the Competition:

The goal of this competition is to detect and translate American Sign Language (ASL) fingerspelling into text. You will create a model trained on the largest dataset of its kind, released specifically for this competition. The data includes more than three million fingerspelled characters produced by over 100 Deaf signers captured via the selfie camera of a smartphone with a variety of backgrounds and lighting conditions.

Your work may help move sign language recognition forward, making AI more accessible for the Deaf and Hard of Hearing community.


Context
Voice-enabled assistants open the world of useful and sometimes life-changing features of modern devices. These revolutionary AI solutions include automated speech recognition (ASR) and machine translation. Unfortunately, these technologies are often not accessible to the more than 70 million Deaf people around the world who use sign language to communicate, nor to the 1.5+ billion people affected by hearing loss globally.

Fingerspelling uses hand shapes that represent individual letters to convey words. While fingerspelling is only a part of ASL, it is often used for communicating names, addresses, phone numbers, and other information commonly entered on a mobile phone. Many Deaf smartphone users can fingerspell words faster than they can type on mobile keyboards. In fact, ASL fingerspelling can be substantially faster than typing on a smartphone’s virtual keyboard (57 words/minute average versus 36 words/minute US average). But sign language recognition AI for text entry lags far behind voice-to-text or even gesture-based typing, as robust datasets didn't previously exist.

Technology that understands sign language fits squarely within Google's mission to organize the world's information and make it universally accessible and useful. Google’s AI principles also support this idea and encourage Google to make products that empower people, widely benefit current and future generations, and work for the common good. This collaboration between Google and the Deaf Professional Arts Network will explore AI solutions that can be scaled globally (such as other sign languages), and support individual user experience needs while interacting with products.

Your participation in this competition could help provide Deaf and Hard of Hearing users the option to fingerspell words instead of using a keyboard. Besides convenient text entry for web search, map directions, and texting, there is potential for an app that can then translate this input using sign language-to-speech technology to speak the words. Such an app would enable the Deaf and Hard of Hearing community to communicate with hearing non-signers more quickly and smoothly.


Evaluation
The evaluation metric for this contest is the normalized total levenshtein distance. Let the total number of characters in all of the labels be N and the total levenshtein distance be D. The metric equals (N - D) / N.

Submission Process
In this competition you will be submitting a TensorFlow Lite model file. The model must take one or more landmark frames as an input and return a float vector (the predicted probabilities of each sign class) as the output. Your model must be packaged into a submission.zip file and compatible with the TensorFlow Lite Runtime v2.14.0, which we are running using Python 3.10. You are welcome to train your model using the framework of your choice, as long as you convert the model checkpoint into the tflite format prior to submission.

Your model must also perform inference in less than 5 hours and use less than 40 MB of storage space. Expect to see approximately 35 hours of video in the test set.

<img width="772" height="260" alt="Screenshot 2025-11-07 at 12 15 52 AM" src="https://github.com/user-attachments/assets/bba72c60-f920-4c12-9cbe-66e40e68cf54" />

<img width="766" height="556" alt="Screenshot 2025-11-07 at 12 16 09 AM" src="https://github.com/user-attachments/assets/846fc4f0-88ac-4626-bd57-c776499416f6" />



Data Card

Dataset Card for the ASL Fingerspelling Recognition Corpus

Dataset Summary

The ASL Fingerspelling Recognition Corpus (version 1.0) is a collection of hand and facial landmarks generated by Mediapipe version 0.9.0.1 on videos of phrases, addresses, phone numbers, and urls fingerspelling by over 100 Deaf signers.

Supported Tasks and Leaderboards

https://www.kaggle.com/competitions/asl-fingerspelling/leaderboard

Languages: Fingerspelling as seen in American Sign Language

Dataset Structure: See https://www.kaggle.com/competitions/asl-fingerspelling/data

Data Splits: Not applicable.

Dataset Creation: Curation Rationale

This dataset includes fingerspelling of letters, numbers, and symbols at real world speeds. This may help move sign language recognition forward, making AI more accessible for the Deaf and Hard of Hearing community.

Source Data

Initial Data Collection and Normalization

Signers who communicate using American Sign Language as their primary language were recruited from across the United States. They were shipped a smartphone with an installed data collection app. The app prompted the signer with the English text to sign. Signers pressed an on-screen button on the phone to begin and end the recording of video during which they fingerspelled the on screen text. The video clip boundaries were adjusted to contain video around the fingerspelling, but this process was not perfectly accurate.

The fingerspelling contains a high degree of co-articulation and even lexicalization of full words. Meaning, the letter handshapes are modified based on what comes before or after. Many variations of handshapes are used to convey meaning. Many different body poses, zoom levels, appearances, and accessories are present in the dataset. Signers could use either their left or right hand to produce fingerspelling, and many even switched between hands in different clips.

The signers were shown text which included capitalization, however, most signers chose not to convey the capitalization within their fingerspelled responses. Some signers conveyed the capitalizations using several different methods, most commonly with one of three methods. Using a curled-L handshape (perhaps with motion to indicate the entire word is capitalized) before the letter to be capitalized, by physically placing the capitalized letters higher in space than the lower case letters, or by shaking the letter's handshape a little bit to indicate it is capitalized. This dataset does not attempt to evaluate the detection of capitalization, and all target phrases are lower case.

Who are the source language producers?

Over 100 signers recruited by the Deaf Professional Arts Network provided the sign. They are from many regions across the United States and all use American Sign Language as their primary form of communication. They represent a mix of skin tones and genders.

Annotation process

Each video was annotated at creation time by the smartphone app. Videos were filtered, modified and clip boundaries assigned through a largely automatic process which corrected many issues in the underlying raw data. Incorrect clip boundaries are the most prevalent remaining issue in the data samples. Little judgment was made on the correctness or quality of the fingerspelling itself.

Who are the annotators?

Google engineers used a mix of manual and automatic processes to improve the dataset quality.

Personal and Sensitive Information

The landmark data has been de-identified. Landmark data should not be used to identify or re-identify an individual. Landmark data is not intended to enable any form of identity recognition or store any unique biometric identification.

Considerations for Using the Data
Discussion of Biases

While ASL is the most common sign language used in the United States, there are many sign languages, including British Sign Language, Native American Sign Languages, Hawaiian Sign Language, French Sign Language, and Signed Exact English. In addition, there are many regional and cultural accents associated with sign in the United States, including Black Sign Language. This dataset focuses on American Sign Language, but it does not capture a representative sample of all the sign variations that would be commonly understood in conversation. ASL has a grammar that is very different from English, and fingerspelling is only a small part of how meaning is conveyed in ASL. A larger number of signers is necessary to better represent skin tones, hand features, and different levels of signing dexterity.

Additional Information
Dataset Curators

The Deaf Professional Arts Network (DPAN), is a 501(c)(3) non-profit founded in 2006 to make music, entertainment, and media accessible.

Licensing Information

The dataset is provided by Google under CC-BY

Contributions

Thanks to the staff at DPAN and their contributors who made this dataset possible. Also, thanks to the students and faculty at Georgia Tech and NTID who collaborated on related datasets.

Acknowledgements

Thanks to the Deaf Professional Arts Network and their community of Deaf signers who made this dataset possible. Thanks also to the students of RIT/NTID who interned at Google to help set this project's agenda and create the data collection tools.

Citation:

Ashley Chow, Glenn Cameron, Manfred Georg, Mark Sherwood, Phil Culliton, Sam Sepah, Sohier Dane, and Thad Starner. Google - American Sign Language Fingerspelling Recognition. https://kaggle.com/competitions/asl-fingerspelling, 2023. Kaggle.
